pipeline:
  name: "ETL Energy Analytics Pipeline"
  purpose: >
    Orchestrates PySpark-based transformations, KPI computation,
    and data quality validation for the Energy Analytics Lakehouse
    using Microsoft Fabric.

  schedule:
    frequency: "Daily"
    time_utc: "02:00"

  steps:
    - order: 1
      name: "Run Energy Transformations"
      type: "Notebook"
      notebook: "nb_transform_energy_data"
      description: >
        Reads Lakehouse tables, applies standardisation,
        builds dimension tables, computes business KPIs,
        and writes curated Delta tables for analytics.
      produces:
        - "dimdate"
        - "dimplant"
        - "factenergydaily"
        - "factheatingdaily"
        - "factco2daily"
        - "fact_energy_kpi_daily"
        - "fact_heating_kpi_daily"
        - "fact_heating_emissions_daily"

    - order: 2
      name: "Validate Curated Tables"
      type: "Notebook"
      notebook: "nb_validate_energy_tables"
      description: >
        Performs data quality checks to ensure curated tables
        exist and contain data. Pipeline fails on validation errors.
      validates:
        - "dimdate"
        - "dimplant"
        - "factenergydaily"
        - "factheatingdaily"
        - "factco2daily"
        - "fact_energy_kpi_daily"
        - "fact_heating_kpi_daily"
        - "fact_heating_emissions_daily"
